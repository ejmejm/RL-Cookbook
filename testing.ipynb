{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import sys\n",
    "\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "sys.path.append('..')\n",
    "\n",
    "from src.agents import RainbowAgent, EzExplorerAgent, SurprisalExplorerAgent\n",
    "from src.agents import SFPredictor\n",
    "from src.agents.Rainbow import DEFAULT_RAINBOW_ARGS\n",
    "from src.envs import *\n",
    "from src.training import *\n",
    "from src.models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = create_simple_gridworld_env(True, 100)\n",
    "env = create_crazy_climber_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_encoder = None\n",
    "if env.observation_space.shape[1] <= 42:\n",
    "  custom_encoder = create_gridworld_convs(env.observation_space.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, obs_dim, n_acts):\n",
    "        super().__init__()\n",
    "        convs = create_atari_convs(obs_dim[0])\n",
    "\n",
    "        test_input = torch.zeros(1, *obs_dim)\n",
    "        with torch.no_grad():\n",
    "            self.encoder_output_size = convs(test_input).view(-1).shape[0]\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            convs,\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(self.encoder_output_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, n_acts))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "class SFNetwork(nn.Module):\n",
    "    def __init__(self, obs_dim, embed_dim=64):\n",
    "        super().__init__()\n",
    "        convs = create_atari_convs(obs_dim[0])\n",
    "\n",
    "        test_input = torch.zeros(1, *obs_dim)\n",
    "        with torch.no_grad():\n",
    "            self.encoder_output_size = convs(test_input).view(-1).shape[0]\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            convs,\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(self.encoder_output_size, embed_dim),\n",
    "            nn.LayerNorm(embed_dim))\n",
    "\n",
    "        self.sf_predictor = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embed_dim, embed_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embed_dim, embed_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeds = self.encoder(x)\n",
    "        sfs = self.sf_predictor(embeds)\n",
    "        return embeds, sfs\n",
    "\n",
    "# sf_model = SFNetwork(list(env.observation_space.shape), 64)\n",
    "# lstate, sfs = sf_model(torch.zeros([2] + list(env.observation_space.shape)))\n",
    "# print(lstate.shape, sfs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 256\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "rainbow_args = copy.copy(DEFAULT_RAINBOW_ARGS)\n",
    "rainbow_args.device = device\n",
    "# rainbow_args.replay_frequency = 8\n",
    "\n",
    "sf_model = SFNetwork(list(env.observation_space.shape), embed_dim)\n",
    "repr_learner = SFPredictor(\n",
    "    sf_model,\n",
    "    batch_size = 32,\n",
    "    update_freq = 16,\n",
    "    log_freq = 200,\n",
    "    target_net_update_freq = 64,\n",
    "    discount_factor = 0.99,\n",
    "    lr = 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_net = PolicyNetwork(list(env.observation_space.shape), env.action_space.n)\n",
    "explore_agent = SurprisalExplorerAgent(env, policy_net, repr_learner, log_freq=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\projects\\rl_representation_learning\\src\\agents\\exploration.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  obs = torch.tensor(obs, dtype=torch.float32, device=self.policy_device)\n",
      "e:\\projects\\rl_representation_learning\\src\\agents\\base.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  [torch.stack([torch.tensor(se, dtype=torch.float32) for se in e], \\\n",
      "e:\\projects\\rl_representation_learning\\src\\agents\\exploration.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  [torch.tensor(e, dtype=torch.float32).to(self.policy_device) \\\n",
      "e:\\projects\\rl_representation_learning\\src\\agents\\exploration.py:124: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  act_idxs = torch.tensor(act_idxs, dtype=torch.long).to(self.policy_device)\n",
      "e:\\projects\\rl_representation_learning\\src\\agents\\representation\\state_prediction.py:83: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  [torch.tensor(e, dtype=torch.float32).to(device) for e in batch_data]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "Step: 160 | Avg policy loss: 234.2974\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "Step: 320 | Avg policy loss: 195.0177\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "Step: 480 | Avg policy loss: 145.3894\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "Step: 640 | Avg policy loss: 89.1556\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "Step: 800 | Avg policy loss: 38.8815\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "Step: 960 | Avg policy loss: 1.2929\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "Step: 1120 | Avg policy loss: 94.7951\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "Step: 1280 | Avg policy loss: 16.3988\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "Step: 1440 | Avg policy loss: -137.5144\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "Step: 1600 | Avg policy loss: -294.0261\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "Step: 1760 | Avg policy loss: -496.1830\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "Step: 1920 | Avg policy loss: -802.4159\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "Step: 2080 | Avg policy loss: -1153.3403\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n",
      "Step: 2240 | Avg policy loss: -1678.3312\n",
      "torch.Size([32]) torch.Size([32]) SHOULD BE THE SAME SHAPE\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-cfd6513c57f0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# explore_agent = EzExplorerAgent(env, repr_learner=repr_learner)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrain_exploration_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexplore_agent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1e6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32me:\\projects\\rl_representation_learning\\src\\training\\simulation.py\u001b[0m in \u001b[0;36mtrain_exploration_model\u001b[1;34m(agent, env, n_steps)\u001b[0m\n\u001b[0;32m      7\u001b[0m   \u001b[0mstep_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m   \u001b[1;32mwhile\u001b[0m \u001b[0mstep_idx\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mn_steps\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mact\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample_act\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0mnext_obs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess_step_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mact\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_obs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\projects\\rl_representation_learning\\src\\agents\\exploration.py\u001b[0m in \u001b[0;36msample_act\u001b[1;34m(self, obs)\u001b[0m\n\u001b[0;32m    102\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_acts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 104\u001b[1;33m     \u001b[0mobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolicy_device\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    105\u001b[0m     \u001b[0mobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# explore_agent = EzExplorerAgent(env, repr_learner=repr_learner)\n",
    "train_exploration_model(explore_agent, env, int(1e6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = RainbowAgent(rainbow_args, env, sf_model.encoder, None) # repr_learner)\n",
    "sf_model = sf_model.to(device)\n",
    "repr_learner._update_target_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 5000\t# Episodes: 1\tAvg ep reward: 3900.00\n",
      "Step: 10000\t# Episodes: 1\tAvg ep reward: 8500.00\n",
      "Step: 15000\t# Episodes: 1\tAvg ep reward: 7700.00\n",
      "Step: 20000\t# Episodes: 2\tAvg ep reward: 5550.00\n",
      "Step: 25000\t# Episodes: 2\tAvg ep reward: 5550.00\n",
      "Step: 30000\t# Episodes: 1\tAvg ep reward: 3100.00\n",
      "Step: 35000\t# Episodes: 2\tAvg ep reward: 8000.00\n",
      "Step: 40000\t# Episodes: 1\tAvg ep reward: 6100.00\n",
      "Step: 45000\t# Episodes: 1\tAvg ep reward: 11000.00\n",
      "Step: 50000\t# Episodes: 1\tAvg ep reward: 11000.00\n",
      "Step: 55000\t# Episodes: 2\tAvg ep reward: 11000.00\n",
      "Step: 60000\t# Episodes: 1\tAvg ep reward: 11000.00\n",
      "Step: 65000\t# Episodes: 2\tAvg ep reward: 11000.00\n",
      "Step: 70000\t# Episodes: 1\tAvg ep reward: 11300.00\n",
      "Step: 75000\t# Episodes: 1\tAvg ep reward: 17100.00\n",
      "Step: 80000\t# Episodes: 2\tAvg ep reward: 20400.00\n",
      "Step: 85000\t# Episodes: 2\tAvg ep reward: 14950.00\n",
      "Step: 90000\t# Episodes: 1\tAvg ep reward: 13900.00\n",
      "Step: 95000\t# Episodes: 1\tAvg ep reward: 17900.00\n",
      "Step: 100000\t# Episodes: 1\tAvg ep reward: 18300.00\n"
     ]
    }
   ],
   "source": [
    "train_task_model(agent, env, int(1e5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #3199 | Repr loss: 0.2400\n",
      "Step #6399 | Repr loss: 0.0778\n",
      "Step #9599 | Repr loss: 0.0721\n",
      "Step #12799 | Repr loss: 0.0637\n",
      "Step #15999 | Repr loss: 0.0634\n",
      "Step #19199 | Repr loss: 0.0572\n",
      "Step #22399 | Repr loss: 0.0541\n",
      "Step #25599 | Repr loss: 0.0511\n",
      "Step #28799 | Repr loss: 0.0568\n",
      "Step #31999 | Repr loss: 0.0477\n",
      "Step #35199 | Repr loss: 0.0433\n",
      "Step #38399 | Repr loss: 0.0404\n",
      "Step #41599 | Repr loss: 0.0376\n",
      "Step #44799 | Repr loss: 0.0358\n",
      "Step #47999 | Repr loss: 0.0334\n",
      "Step #51199 | Repr loss: 0.0316\n",
      "Step #54399 | Repr loss: 0.0355\n",
      "Step #57599 | Repr loss: 0.0303\n",
      "Step #60799 | Repr loss: 0.0276\n",
      "Step #63999 | Repr loss: 0.0257\n",
      "Step #67199 | Repr loss: 0.0240\n",
      "Step #70399 | Repr loss: 0.0223\n",
      "Step #73599 | Repr loss: 0.0209\n",
      "Step #76799 | Repr loss: 0.0200\n",
      "Step #79999 | Repr loss: 0.0225\n",
      "Step #83199 | Repr loss: 0.0196\n",
      "Step #86399 | Repr loss: 0.0176\n",
      "Step #89599 | Repr loss: 0.0162\n",
      "Step #92799 | Repr loss: 0.0150\n",
      "Step #95999 | Repr loss: 0.0142\n",
      "Step #99199 | Repr loss: 0.0136\n"
     ]
    }
   ],
   "source": [
    "sf_model = SFNetwork(list(env.observation_space.shape), embed_dim)\n",
    "repr_learner = SFPredictor(\n",
    "    sf_model,\n",
    "    batch_size = 32,\n",
    "    update_freq = 16,\n",
    "    log_freq = 200,\n",
    "    target_net_update_freq = 64,\n",
    "    discount_factor = 0.99,\n",
    "    lr = 1e-4)\n",
    "\n",
    "explore_agent = EzExplorerAgent(env, repr_learner=repr_learner)\n",
    "train_exploration_model(explore_agent, env, int(1e5))\n",
    "agent = RainbowAgent(rainbow_args, env, sf_model.encoder, None) # repr_learner)\n",
    "sf_model = sf_model.to(device)\n",
    "repr_learner._update_target_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 5000\t# Episodes: 1\tAvg ep reward: 800.00\n",
      "Step: 10000\t# Episodes: 1\tAvg ep reward: 1200.00\n",
      "Step: 15000\t# Episodes: 1\tAvg ep reward: 1300.00\n",
      "Step: 20000\t# Episodes: 2\tAvg ep reward: 5200.00\n",
      "Step: 25000\t# Episodes: 1\tAvg ep reward: 4300.00\n",
      "Step: 30000\t# Episodes: 2\tAvg ep reward: 1850.00\n",
      "Step: 35000\t# Episodes: 1\tAvg ep reward: 2700.00\n",
      "Step: 40000\t# Episodes: 1\tAvg ep reward: 3400.00\n",
      "Step: 45000\t# Episodes: 2\tAvg ep reward: 4000.00\n",
      "Step: 50000\t# Episodes: 1\tAvg ep reward: 6200.00\n",
      "Step: 55000\t# Episodes: 1\tAvg ep reward: 7800.00\n",
      "Step: 60000\t# Episodes: 2\tAvg ep reward: 7100.00\n",
      "Step: 65000\t# Episodes: 2\tAvg ep reward: 7100.00\n",
      "Step: 70000\t# Episodes: 1\tAvg ep reward: 9600.00\n",
      "Step: 75000\t# Episodes: 1\tAvg ep reward: 9100.00\n",
      "Step: 80000\t# Episodes: 2\tAvg ep reward: 10950.00\n",
      "Step: 85000\t# Episodes: 1\tAvg ep reward: 9300.00\n",
      "Step: 90000\t# Episodes: 2\tAvg ep reward: 11000.00\n",
      "Step: 95000\t# Episodes: 2\tAvg ep reward: 11000.00\n",
      "Step: 100000\t# Episodes: 1\tAvg ep reward: 11000.00\n"
     ]
    }
   ],
   "source": [
    "train_task_model(agent, env, int(1e5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------\n",
      "With representation learning\n",
      "----------------\n",
      "Starting new training loop\n",
      "Step: 5000\t# Episodes: 1\tAvg ep reward: 10400.00\n",
      "Step: 10000\t# Episodes: 1\tAvg ep reward: 7000.00\n",
      "Step: 15000\t# Episodes: 1\tAvg ep reward: 4100.00\n",
      "Step: 20000\t# Episodes: 2\tAvg ep reward: 7900.00\n",
      "Step: 25000\t# Episodes: 1\tAvg ep reward: 6000.00\n",
      "Step: 30000\t# Episodes: 1\tAvg ep reward: 3100.00\n",
      "Step: 35000\t# Episodes: 2\tAvg ep reward: 3850.00\n",
      "Step: 40000\t# Episodes: 1\tAvg ep reward: 6000.00\n",
      "Step: 45000\t# Episodes: 2\tAvg ep reward: 6250.00\n",
      "Step: 50000\t# Episodes: 2\tAvg ep reward: 10950.00\n",
      "Step: 55000\t# Episodes: 2\tAvg ep reward: 12600.00\n",
      "Step: 60000\t# Episodes: 2\tAvg ep reward: 7300.00\n",
      "Step: 65000\t# Episodes: 1\tAvg ep reward: 24400.00\n",
      "Step: 70000\t# Episodes: 1\tAvg ep reward: 24800.00\n",
      "Step: 75000\t# Episodes: 2\tAvg ep reward: 24600.00\n",
      "Step: 80000\t# Episodes: 2\tAvg ep reward: 15700.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ninja\\.conda\\envs\\ml2\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3440: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\ninja\\.conda\\envs\\ml2\\lib\\site-packages\\numpy\\core\\_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 85000\t# Episodes: 0\tAvg ep reward: nan\n",
      "Step: 90000\t# Episodes: 2\tAvg ep reward: 14900.00\n",
      "Step: 95000\t# Episodes: 2\tAvg ep reward: 24800.00\n",
      "Step: 100000\t# Episodes: 1\tAvg ep reward: 24900.00\n",
      "Starting new training loop\n",
      "Step: 5000\t# Episodes: 1\tAvg ep reward: 3400.00\n",
      "Step: 10000\t# Episodes: 1\tAvg ep reward: 5200.00\n",
      "Step: 15000\t# Episodes: 2\tAvg ep reward: 5200.00\n",
      "Step: 20000\t# Episodes: 1\tAvg ep reward: 3500.00\n",
      "Step: 25000\t# Episodes: 2\tAvg ep reward: 3550.00\n",
      "Step: 30000\t# Episodes: 2\tAvg ep reward: 3450.00\n",
      "Step: 35000\t# Episodes: 1\tAvg ep reward: 4200.00\n",
      "Step: 40000\t# Episodes: 2\tAvg ep reward: 4100.00\n",
      "Step: 45000\t# Episodes: 1\tAvg ep reward: 5300.00\n",
      "Step: 50000\t# Episodes: 2\tAvg ep reward: 5350.00\n",
      "Step: 55000\t# Episodes: 1\tAvg ep reward: 1400.00\n",
      "Step: 60000\t# Episodes: 1\tAvg ep reward: 4100.00\n",
      "Step: 65000\t# Episodes: 1\tAvg ep reward: 9500.00\n",
      "Step: 70000\t# Episodes: 2\tAvg ep reward: 3400.00\n",
      "Step: 75000\t# Episodes: 1\tAvg ep reward: 3400.00\n",
      "Step: 80000\t# Episodes: 2\tAvg ep reward: 4650.00\n",
      "Step: 85000\t# Episodes: 2\tAvg ep reward: 7500.00\n",
      "Step: 90000\t# Episodes: 1\tAvg ep reward: 6800.00\n",
      "Step: 95000\t# Episodes: 1\tAvg ep reward: 11000.00\n",
      "Step: 100000\t# Episodes: 0\tAvg ep reward: nan\n",
      "Starting new training loop\n",
      "Step: 5000\t# Episodes: 0\tAvg ep reward: nan\n",
      "Step: 10000\t# Episodes: 1\tAvg ep reward: 11000.00\n",
      "Step: 15000\t# Episodes: 2\tAvg ep reward: 1900.00\n",
      "Step: 20000\t# Episodes: 1\tAvg ep reward: 1900.00\n",
      "Step: 25000\t# Episodes: 1\tAvg ep reward: 400.00\n",
      "Step: 30000\t# Episodes: 1\tAvg ep reward: 4000.00\n",
      "Step: 35000\t# Episodes: 1\tAvg ep reward: 1400.00\n",
      "Step: 40000\t# Episodes: 1\tAvg ep reward: 2300.00\n",
      "Step: 45000\t# Episodes: 1\tAvg ep reward: 1900.00\n",
      "Step: 50000\t# Episodes: 2\tAvg ep reward: 1700.00\n",
      "Step: 55000\t# Episodes: 1\tAvg ep reward: 1800.00\n",
      "Step: 60000\t# Episodes: 1\tAvg ep reward: 1300.00\n",
      "Step: 65000\t# Episodes: 1\tAvg ep reward: 1000.00\n",
      "Step: 70000\t# Episodes: 0\tAvg ep reward: nan\n",
      "Step: 75000\t# Episodes: 1\tAvg ep reward: 3500.00\n",
      "Step: 80000\t# Episodes: 2\tAvg ep reward: 3700.00\n",
      "Step: 85000\t# Episodes: 1\tAvg ep reward: 6000.00\n",
      "Step: 90000\t# Episodes: 2\tAvg ep reward: 5100.00\n",
      "Step: 95000\t# Episodes: 2\tAvg ep reward: 5150.00\n",
      "Step: 100000\t# Episodes: 2\tAvg ep reward: 8750.00\n",
      "Starting new training loop\n",
      "Step: 5000\t# Episodes: 1\tAvg ep reward: 3400.00\n",
      "Step: 10000\t# Episodes: 1\tAvg ep reward: 6600.00\n",
      "Step: 15000\t# Episodes: 2\tAvg ep reward: 5950.00\n",
      "Step: 20000\t# Episodes: 2\tAvg ep reward: 3950.00\n",
      "Step: 25000\t# Episodes: 1\tAvg ep reward: 7000.00\n",
      "Step: 30000\t# Episodes: 2\tAvg ep reward: 4850.00\n",
      "Step: 35000\t# Episodes: 1\tAvg ep reward: 3200.00\n",
      "Step: 40000\t# Episodes: 1\tAvg ep reward: 3000.00\n",
      "Step: 45000\t# Episodes: 2\tAvg ep reward: 5600.00\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-feadbdc080df>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mrepr_learner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_target_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m     \u001b[0mtrain_task_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1e5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'-'\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m16\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'\\nWithout representation learning\\n'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'-'\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\projects\\rl_representation_learning\\src\\training\\simulation.py\u001b[0m in \u001b[0;36mtrain_task_model\u001b[1;34m(agent, env, n_steps)\u001b[0m\n\u001b[0;32m     38\u001b[0m       \u001b[0mobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext_obs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m     \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m     \u001b[0mstep_idx\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\projects\\rl_representation_learning\\src\\agents\\rainbow.py\u001b[0m in \u001b[0;36mend_step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     95\u001b[0m       \u001b[1;31m# Train with n-step distributional double-Q learning\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep_idx\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplay_frequency\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 97\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdqn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     98\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep_idx\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluation_interval\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\projects\\rl_representation_learning\\src\\agents\\Rainbow\\src\\agent.py\u001b[0m in \u001b[0;36mlearn\u001b[1;34m(self, mem)\u001b[0m\n\u001b[0;32m     95\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[1;31m# Sample transitions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 97\u001b[1;33m     \u001b[0midxs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnonterminals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     98\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m     \u001b[1;31m# Calculate current state probabilities (online network noise already sampled)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\projects\\rl_representation_learning\\src\\agents\\Rainbow\\src\\memory.py\u001b[0m in \u001b[0;36msample\u001b[1;34m(self, batch_size)\u001b[0m\n\u001b[0;32m    147\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mp_total\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransitions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtotal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Retrieve sum of all priorities (used to create a normalised probability distribution)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 149\u001b[1;33m     \u001b[0mprobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midxs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtree_idxs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnonterminals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_samples_from_segments\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp_total\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Get batch of valid samples\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    150\u001b[0m     \u001b[0mprobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprobs\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mp_total\u001b[0m  \u001b[1;31m# Calculate normalised probabilities\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[0mcapacity\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcapacity\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransitions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfull\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransitions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\projects\\rl_representation_learning\\src\\agents\\Rainbow\\src\\memory.py\u001b[0m in \u001b[0;36m_get_samples_from_segments\u001b[1;34m(self, batch_size, p_total)\u001b[0m\n\u001b[0;32m    134\u001b[0m     \u001b[1;31m# Create un-discretised states and nth next states\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m     \u001b[0mall_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransitions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'state'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 136\u001b[1;33m     \u001b[0mstates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_states\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiv_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m255\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    137\u001b[0m     \u001b[0mnext_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_states\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiv_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m255\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m     \u001b[1;31m# Discrete actions to be used as index\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print('-'*16 + '\\nWith representation learning\\n' + '-'*16)\n",
    "for _ in range(5):\n",
    "    print('Starting new training loop')\n",
    "\n",
    "    embed_dim = 64\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    rainbow_args = copy.copy(DEFAULT_RAINBOW_ARGS)\n",
    "    rainbow_args.device = device\n",
    "    # rainbow_args.replay_frequency = 8\n",
    "\n",
    "    sf_model = SFNetwork(list(env.observation_space.shape), embed_dim)\n",
    "    repr_learner = SFPredictor(sf_model, lr=1e-4)\n",
    "\n",
    "    agent = RainbowAgent(rainbow_args, env, sf_model.encoder, repr_learner)\n",
    "    sf_model = sf_model.to(device)\n",
    "    repr_learner._update_target_model()\n",
    "\n",
    "    train_task_model(agent, env, int(1e5))\n",
    "\n",
    "print('-'*16 + '\\nWithout representation learning\\n' + '-'*16)\n",
    "for _ in range(5):\n",
    "    print('Starting new training loop')\n",
    "\n",
    "    embed_dim = 64\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    rainbow_args = copy.copy(DEFAULT_RAINBOW_ARGS)\n",
    "    rainbow_args.device = device\n",
    "    # rainbow_args.replay_frequency = 8\n",
    "\n",
    "    sf_model = SFNetwork(list(env.observation_space.shape), embed_dim)\n",
    "    repr_learner = SFPredictor(sf_model, lr=1e-4)\n",
    "\n",
    "    agent = RainbowAgent(rainbow_args, env, sf_model.encoder, None)\n",
    "    sf_model = sf_model.to(device)\n",
    "    repr_learner._update_target_model()\n",
    "\n",
    "    train_task_model(agent, env, int(1e5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent = TestRL(agent)\n",
    "# agent.start_task(1000)\n",
    "# obs = env.reset()\n",
    "# act = agent.sample_act(obs)\n",
    "# print('Act:', act)\n",
    "# obs, reward, done, _ = env.step(act)\n",
    "# agent.process_step_data((obs, act, reward, obs, done))\n",
    "# agent.end_step()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6717cf457fe527f2ad07ab71b4770f157b357bf37d07e7427487ba89b10c0212"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('ml2': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
