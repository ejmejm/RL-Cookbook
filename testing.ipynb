{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import sys\n",
    "\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "sys.path.append('..')\n",
    "\n",
    "from src.agents import RainbowAgent, EzExplorerAgent, SurprisalExplorerAgent\n",
    "from src.agents import SFPredictor\n",
    "from src.agents.Rainbow import DEFAULT_RAINBOW_ARGS\n",
    "from src.envs import *\n",
    "from src.training import *\n",
    "from src.models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = create_simple_gridworld_env(False, 100)\n",
    "# env = create_crazy_climber_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_encoder = None\n",
    "if env.observation_space.shape[1] <= 42:\n",
    "  custom_encoder = create_gridworld_convs(env.observation_space.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if env.observation_space.shape[1] <= 32:\n",
    "    CREATE_CONV_FUNC = create_gridworld_convs\n",
    "else:\n",
    "    CREATE_CONV_FUNC = create_atari_convs\n",
    "\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, obs_dim, n_acts):\n",
    "        super().__init__()\n",
    "        convs = CREATE_CONV_FUNC(obs_dim[0])\n",
    "\n",
    "        test_input = torch.zeros(1, *obs_dim)\n",
    "        with torch.no_grad():\n",
    "            self.encoder_output_size = convs(test_input).view(-1).shape[0]\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            convs,\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(self.encoder_output_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, n_acts))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "class CriticNetwork(nn.Module):\n",
    "    def __init__(self, obs_dim):\n",
    "        super().__init__()\n",
    "        convs = CREATE_CONV_FUNC(obs_dim[0])\n",
    "\n",
    "        test_input = torch.zeros(1, *obs_dim)\n",
    "        with torch.no_grad():\n",
    "            self.encoder_output_size = convs(test_input).view(-1).shape[0]\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            convs,\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(self.encoder_output_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "class SFNetwork(nn.Module):\n",
    "    def __init__(self, obs_dim, embed_dim=64):\n",
    "        super().__init__()\n",
    "        convs = CREATE_CONV_FUNC(obs_dim[0])\n",
    "\n",
    "        test_input = torch.zeros(1, *obs_dim)\n",
    "        with torch.no_grad():\n",
    "            self.encoder_output_size = convs(test_input).view(-1).shape[0]\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            convs,\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(self.encoder_output_size, embed_dim),\n",
    "            nn.LayerNorm(embed_dim))\n",
    "\n",
    "        self.sf_predictor = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embed_dim, embed_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embed_dim, embed_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeds = self.encoder(x)\n",
    "        sfs = self.sf_predictor(embeds)\n",
    "        return embeds, sfs\n",
    "\n",
    "# sf_model = SFNetwork(list(env.observation_space.shape), 64)\n",
    "# lstate, sfs = sf_model(torch.zeros([2] + list(env.observation_space.shape)))\n",
    "# print(lstate.shape, sfs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 256\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "rainbow_args = copy.copy(DEFAULT_RAINBOW_ARGS)\n",
    "rainbow_args.device = device\n",
    "# rainbow_args.replay_frequency = 8\n",
    "\n",
    "sf_model = SFNetwork(list(env.observation_space.shape), embed_dim)\n",
    "sf_model = sf_model.to(device)\n",
    "repr_learner = SFPredictor(\n",
    "    sf_model,\n",
    "    batch_size = 32,\n",
    "    update_freq = 16,\n",
    "    log_freq = 200,\n",
    "    target_net_update_freq = 64,\n",
    "    discount_factor = 0.99,\n",
    "    lr = 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_net = PolicyNetwork(list(env.observation_space.shape), env.action_space.n)\n",
    "policy_net = policy_net.to(device)\n",
    "critic_net = CriticNetwork(list(env.observation_space.shape))\n",
    "critic_net = critic_net.to(device)\n",
    "\n",
    "explore_agent = SurprisalExplorerAgent(\n",
    "    env, policy_net, critic_net, repr_learner, log_freq=50,\n",
    "    update_freq=2000, batch_size=2000)\n",
    "# train_exploration_model(explore_agent, env, int(1e6))\n",
    "train_task_model(explore_agent, env, int(1e6), print_rewards=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_copy = copy.deepcopy(sf_model.encoder)\n",
    "encoder_copy = encoder_copy.to('cpu')\n",
    "\n",
    "agent = RainbowAgent(rainbow_args, env, encoder_copy, repr_learner=None)\n",
    "sf_model = sf_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 5000\t# Episodes: 50\tAvg ep reward: 0.06\n",
      "Step: 10000\t# Episodes: 877\tAvg ep reward: 0.99\n",
      "Step: 15000\t# Episodes: 1000\tAvg ep reward: 1.00\n",
      "Step: 20000\t# Episodes: 1000\tAvg ep reward: 1.00\n",
      "Step: 25000\t# Episodes: 1000\tAvg ep reward: 1.00\n",
      "Step: 30000\t# Episodes: 1000\tAvg ep reward: 1.00\n",
      "Step: 35000\t# Episodes: 1000\tAvg ep reward: 1.00\n",
      "Step: 40000\t# Episodes: 1000\tAvg ep reward: 1.00\n",
      "Step: 45000\t# Episodes: 1000\tAvg ep reward: 1.00\n",
      "Step: 50000\t# Episodes: 1000\tAvg ep reward: 1.00\n",
      "Step: 55000\t# Episodes: 1000\tAvg ep reward: 1.00\n",
      "Step: 60000\t# Episodes: 1000\tAvg ep reward: 1.00\n",
      "Step: 65000\t# Episodes: 1000\tAvg ep reward: 1.00\n",
      "Step: 70000\t# Episodes: 1000\tAvg ep reward: 1.00\n",
      "Step: 75000\t# Episodes: 1000\tAvg ep reward: 1.00\n",
      "Step: 80000\t# Episodes: 1000\tAvg ep reward: 1.00\n",
      "Step: 85000\t# Episodes: 1000\tAvg ep reward: 1.00\n",
      "Step: 90000\t# Episodes: 1000\tAvg ep reward: 1.00\n",
      "Step: 95000\t# Episodes: 1000\tAvg ep reward: 1.00\n",
      "Step: 100000\t# Episodes: 1000\tAvg ep reward: 1.00\n"
     ]
    }
   ],
   "source": [
    "train_task_model(agent, env, int(1e5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6717cf457fe527f2ad07ab71b4770f157b357bf37d07e7427487ba89b10c0212"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('ml2': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
