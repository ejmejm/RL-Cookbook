{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import sys\n",
    "\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "sys.path.append('..')\n",
    "\n",
    "from src.agents import RainbowAgent, EzExplorerAgent\n",
    "from src.agents import SFPredictor\n",
    "from src.agents.Rainbow import DEFAULT_RAINBOW_ARGS\n",
    "from src.envs import *\n",
    "from src.training import *\n",
    "from src.models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = create_simple_gridworld_env(True, 100)\n",
    "env = create_crazy_climber_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_encoder = None\n",
    "if env.observation_space.shape[1] <= 42:\n",
    "  custom_encoder = create_gridworld_convs(env.observation_space.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SFNetwork(nn.Module):\n",
    "    def __init__(self, obs_dim, embed_dim=64):\n",
    "        super().__init__()\n",
    "        convs = create_atari_convs(obs_dim[0])\n",
    "        # self.add_module('convs', self.convs)\n",
    "\n",
    "        test_input = torch.zeros(1, *obs_dim)\n",
    "        with torch.no_grad():\n",
    "            self.encoder_output_size = convs(test_input).view(-1).shape[0]\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            convs,\n",
    "            # nn.Dropout2d(0.6),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(self.encoder_output_size, embed_dim))\n",
    "\n",
    "\n",
    "        self.sf_predictor = nn.Sequential(\n",
    "        nn.Linear(embed_dim, embed_dim),\n",
    "        # nn.Dropout(0.6),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(embed_dim, embed_dim),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(embed_dim, embed_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        embeds = F.normalize(self.sf_predictor(z), dim=-1)\n",
    "        sfs = self.sf_predictor(embeds)\n",
    "        return embeds, sfs\n",
    "\n",
    "    def encode_obs(self, x):\n",
    "        return F.normalize(self.encoder(x), dim=-1)\n",
    "\n",
    "# sf_model = SFNetwork(list(env.observation_space.shape), 64)\n",
    "# lstate, sfs = sf_model(torch.zeros([2] + list(env.observation_space.shape)))\n",
    "# print(lstate.shape, sfs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target model updated!\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 256\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "rainbow_args = copy.copy(DEFAULT_RAINBOW_ARGS)\n",
    "rainbow_args.device = device\n",
    "# rainbow_args.replay_frequency = 8\n",
    "\n",
    "sf_model = SFNetwork(list(env.observation_space.shape), embed_dim)\n",
    "repr_learner = SFPredictor(\n",
    "    sf_model,\n",
    "    batch_size = 32,\n",
    "    update_freq = 16,\n",
    "    log_freq = 200,\n",
    "    target_net_update_freq = 64,\n",
    "    discount_factor = 0.99,\n",
    "    lr = 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\projects\\rl_representation_learning\\src\\agents\\base.py:78: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  [torch.stack([torch.tensor(se, dtype=torch.float32) for se in e], \\\n",
      "e:\\projects\\rl_representation_learning\\src\\agents\\representation\\state_prediction.py:78: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  [torch.tensor(e, dtype=torch.float32).to(device) for e in batch_data]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target model updated!\n",
      "Representation loss: 0.0015\n",
      "Target model updated!\n",
      "Target model updated!\n",
      "Representation loss: 0.0006\n",
      "Target model updated!\n",
      "Representation loss: 0.0004\n",
      "Target model updated!\n",
      "Target model updated!\n",
      "Representation loss: 0.0004\n",
      "Target model updated!\n",
      "Representation loss: 0.0002\n",
      "Target model updated!\n",
      "Target model updated!\n",
      "Representation loss: 0.0004\n",
      "Target model updated!\n",
      "Representation loss: 0.0002\n",
      "Target model updated!\n",
      "Target model updated!\n",
      "Representation loss: 0.0003\n",
      "Target model updated!\n",
      "Target model updated!\n",
      "Representation loss: 0.0002\n",
      "Target model updated!\n",
      "Representation loss: 0.0002\n",
      "Target model updated!\n",
      "Target model updated!\n",
      "Representation loss: 0.0003\n",
      "Target model updated!\n",
      "Representation loss: 0.0002\n",
      "Target model updated!\n",
      "Target model updated!\n",
      "Representation loss: 0.0003\n",
      "Target model updated!\n",
      "Representation loss: 0.0002\n",
      "Target model updated!\n",
      "Target model updated!\n",
      "Representation loss: 0.0003\n",
      "Target model updated!\n",
      "Target model updated!\n",
      "Representation loss: 0.0001\n",
      "Target model updated!\n",
      "Representation loss: 0.0003\n",
      "Target model updated!\n",
      "Target model updated!\n",
      "Representation loss: 0.0002\n",
      "Target model updated!\n",
      "Representation loss: 0.0003\n",
      "Target model updated!\n",
      "Target model updated!\n",
      "Representation loss: 0.0002\n",
      "Target model updated!\n",
      "Representation loss: 0.0001\n",
      "Target model updated!\n",
      "Target model updated!\n",
      "Representation loss: 0.0002\n",
      "Target model updated!\n",
      "Representation loss: 0.0001\n",
      "Target model updated!\n",
      "Target model updated!\n",
      "Representation loss: 0.0002\n",
      "Target model updated!\n",
      "Target model updated!\n",
      "Representation loss: 0.0001\n",
      "Target model updated!\n",
      "Representation loss: 0.0002\n",
      "Target model updated!\n",
      "Target model updated!\n",
      "Representation loss: 0.0002\n",
      "Target model updated!\n",
      "Representation loss: 0.0001\n",
      "Target model updated!\n",
      "Target model updated!\n",
      "Representation loss: 0.0002\n",
      "Target model updated!\n",
      "Representation loss: 0.0001\n",
      "Target model updated!\n",
      "Target model updated!\n",
      "Representation loss: 0.0002\n",
      "Target model updated!\n",
      "Target model updated!\n",
      "Representation loss: 0.0001\n",
      "Target model updated!\n",
      "Representation loss: 0.0002\n",
      "Target model updated!\n",
      "Target model updated!\n",
      "Representation loss: 0.0001\n",
      "Target model updated!\n",
      "Representation loss: 0.0001\n",
      "Target model updated!\n",
      "Target model updated!\n",
      "Representation loss: 0.0001\n",
      "Target model updated!\n",
      "Representation loss: 0.0001\n",
      "Target model updated!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-6e2b8e77c434>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mexplore_agent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEzExplorerAgent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrepr_learner\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrepr_learner\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrain_exploration_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexplore_agent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1e5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32me:\\projects\\rl_representation_learning\\src\\training\\simulation.py\u001b[0m in \u001b[0;36mtrain_exploration_model\u001b[1;34m(agent, env, n_steps)\u001b[0m\n\u001b[0;32m      8\u001b[0m   \u001b[1;32mwhile\u001b[0m \u001b[0mstep_idx\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mn_steps\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mact\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample_act\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mnext_obs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess_step_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mact\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_obs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\ml2\\lib\\site-packages\\gym\\core.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    321\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    322\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 323\u001b[1;33m         \u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    324\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\ml2\\lib\\site-packages\\gym\\wrappers\\frame_stack.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 115\u001b[1;33m         \u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    116\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\ml2\\lib\\site-packages\\gym\\wrappers\\atari_preprocessing.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    126\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0male\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetScreenRGB\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobs_buffer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_obs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\ml2\\lib\\site-packages\\gym\\wrappers\\atari_preprocessing.py\u001b[0m in \u001b[0;36m_get_obs\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_obs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframe_skip\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# more efficient in-place pooling\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m             \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaximum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobs_buffer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobs_buffer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobs_buffer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    154\u001b[0m         obs = cv2.resize(\n\u001b[0;32m    155\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobs_buffer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "explore_agent = EzExplorerAgent(env, repr_learner=repr_learner)\n",
    "train_exploration_model(explore_agent, env, int(1e5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = RainbowAgent(rainbow_args, env, sf_model.encoder, None) # repr_learner)\n",
    "sf_model = sf_model.to(device)\n",
    "repr_learner._update_target_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-f0399af0f630>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_task_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1e5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32me:\\projects\\rl_representation_learning\\src\\training\\simulation.py\u001b[0m in \u001b[0;36mtrain_task_model\u001b[1;34m(agent, env, n_steps)\u001b[0m\n\u001b[0;32m     28\u001b[0m   \u001b[0mstep_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m   \u001b[1;32mwhile\u001b[0m \u001b[0mstep_idx\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mn_steps\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m     \u001b[0mact\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample_act\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m     \u001b[0mnext_obs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess_step_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mact\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_obs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\projects\\rl_representation_learning\\src\\agents\\rainbow.py\u001b[0m in \u001b[0;36msample_act\u001b[1;34m(self, obs)\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mobs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m       \u001b[0mobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m     \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdqn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Choose an action greedily (with noisy weights)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\projects\\rl_representation_learning\\src\\agents\\Rainbow\\src\\agent.py\u001b[0m in \u001b[0;36mact\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     87\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0monline_net\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msupport\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m   \u001b[1;31m# Acts with an Îµ-greedy policy (used for evaluation only)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\ml2\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\projects\\rl_representation_learning\\src\\agents\\Rainbow\\src\\model.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, log)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv_output_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m     \u001b[0mv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc_z_v\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc_h_v\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Value stream\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\ml2\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\ml2\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    137\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 139\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    140\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\ml2\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\ml2\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    137\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 139\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    140\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\ml2\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\ml2\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    441\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 443\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    445\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\ml2\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    437\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    438\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m--> 439\u001b[1;33m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[0;32m    440\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[0;32m    441\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_task_model(agent, env, int(1e5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------\n",
      "With representation learning\n",
      "----------------\n",
      "Starting new training loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\projects\\rl_representation_learning\\src\\agents\\representation\\state_prediction.py:76: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  [torch.stack([torch.tensor(se, dtype=torch.float32) for se in e], \\\n",
      "C:\\Users\\ninja\\.conda\\envs\\ml2\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3440: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\ninja\\.conda\\envs\\ml2\\lib\\site-packages\\numpy\\core\\_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 5000\t# Episodes: 0\tAvg ep reward: nan\n",
      "Step: 10000\t# Episodes: 2\tAvg ep reward: 5400.00\n",
      "Step: 15000\t# Episodes: 2\tAvg ep reward: 6000.00\n",
      "Step: 20000\t# Episodes: 1\tAvg ep reward: 4900.00\n",
      "Step: 25000\t# Episodes: 2\tAvg ep reward: 10200.00\n",
      "Step: 30000\t# Episodes: 2\tAvg ep reward: 5900.00\n",
      "Step: 35000\t# Episodes: 2\tAvg ep reward: 6650.00\n",
      "Step: 40000\t# Episodes: 2\tAvg ep reward: 5350.00\n",
      "Step: 45000\t# Episodes: 2\tAvg ep reward: 5300.00\n",
      "Step: 50000\t# Episodes: 2\tAvg ep reward: 6400.00\n",
      "Step: 55000\t# Episodes: 1\tAvg ep reward: 6800.00\n",
      "Step: 60000\t# Episodes: 1\tAvg ep reward: 11000.00\n",
      "Step: 65000\t# Episodes: 1\tAvg ep reward: 8100.00\n",
      "Step: 70000\t# Episodes: 1\tAvg ep reward: 7200.00\n",
      "Step: 75000\t# Episodes: 2\tAvg ep reward: 3850.00\n",
      "Step: 80000\t# Episodes: 1\tAvg ep reward: 4100.00\n",
      "Step: 85000\t# Episodes: 1\tAvg ep reward: 3400.00\n",
      "Step: 90000\t# Episodes: 2\tAvg ep reward: 5300.00\n",
      "Step: 95000\t# Episodes: 1\tAvg ep reward: 4400.00\n",
      "Step: 100000\t# Episodes: 1\tAvg ep reward: 3400.00\n",
      "Starting new training loop\n",
      "Step: 5000\t# Episodes: 1\tAvg ep reward: 3400.00\n",
      "Step: 10000\t# Episodes: 1\tAvg ep reward: 3400.00\n",
      "Step: 15000\t# Episodes: 2\tAvg ep reward: 6000.00\n",
      "Step: 20000\t# Episodes: 2\tAvg ep reward: 4900.00\n",
      "Step: 25000\t# Episodes: 1\tAvg ep reward: 7400.00\n",
      "Step: 30000\t# Episodes: 2\tAvg ep reward: 5600.00\n",
      "Step: 35000\t# Episodes: 1\tAvg ep reward: 7900.00\n",
      "Step: 40000\t# Episodes: 1\tAvg ep reward: 3400.00\n",
      "Step: 45000\t# Episodes: 2\tAvg ep reward: 5350.00\n",
      "Step: 50000\t# Episodes: 2\tAvg ep reward: 3600.00\n",
      "Step: 55000\t# Episodes: 1\tAvg ep reward: 6000.00\n",
      "Step: 60000\t# Episodes: 2\tAvg ep reward: 5900.00\n",
      "Step: 65000\t# Episodes: 1\tAvg ep reward: 6500.00\n",
      "Step: 70000\t# Episodes: 2\tAvg ep reward: 6500.00\n",
      "Step: 75000\t# Episodes: 1\tAvg ep reward: 7900.00\n",
      "Step: 80000\t# Episodes: 2\tAvg ep reward: 6050.00\n",
      "Step: 85000\t# Episodes: 1\tAvg ep reward: 6000.00\n",
      "Step: 90000\t# Episodes: 1\tAvg ep reward: 7000.00\n",
      "Step: 95000\t# Episodes: 2\tAvg ep reward: 6550.00\n",
      "Step: 100000\t# Episodes: 1\tAvg ep reward: 7400.00\n",
      "Starting new training loop\n",
      "Step: 5000\t# Episodes: 0\tAvg ep reward: nan\n",
      "Step: 10000\t# Episodes: 2\tAvg ep reward: 3400.00\n",
      "Step: 15000\t# Episodes: 1\tAvg ep reward: 1200.00\n",
      "Step: 20000\t# Episodes: 1\tAvg ep reward: 7300.00\n",
      "Step: 25000\t# Episodes: 2\tAvg ep reward: 8250.00\n",
      "Step: 30000\t# Episodes: 1\tAvg ep reward: 3400.00\n",
      "Step: 35000\t# Episodes: 2\tAvg ep reward: 5250.00\n",
      "Step: 40000\t# Episodes: 0\tAvg ep reward: nan\n",
      "Step: 45000\t# Episodes: 2\tAvg ep reward: 7200.00\n",
      "Step: 50000\t# Episodes: 1\tAvg ep reward: 3400.00\n",
      "Step: 55000\t# Episodes: 1\tAvg ep reward: 10800.00\n",
      "Step: 60000\t# Episodes: 2\tAvg ep reward: 3400.00\n",
      "Step: 65000\t# Episodes: 1\tAvg ep reward: 3400.00\n",
      "Step: 70000\t# Episodes: 1\tAvg ep reward: 3400.00\n",
      "Step: 75000\t# Episodes: 1\tAvg ep reward: 3400.00\n",
      "Step: 80000\t# Episodes: 2\tAvg ep reward: 3400.00\n",
      "Step: 85000\t# Episodes: 0\tAvg ep reward: nan\n",
      "Step: 90000\t# Episodes: 2\tAvg ep reward: 3600.00\n",
      "Step: 95000\t# Episodes: 1\tAvg ep reward: 3400.00\n",
      "Step: 100000\t# Episodes: 2\tAvg ep reward: 3000.00\n",
      "Starting new training loop\n",
      "Step: 5000\t# Episodes: 1\tAvg ep reward: 2100.00\n",
      "Step: 10000\t# Episodes: 1\tAvg ep reward: 3400.00\n",
      "Step: 15000\t# Episodes: 1\tAvg ep reward: 4200.00\n",
      "Step: 20000\t# Episodes: 1\tAvg ep reward: 4100.00\n",
      "Step: 25000\t# Episodes: 2\tAvg ep reward: 5550.00\n",
      "Step: 30000\t# Episodes: 1\tAvg ep reward: 3900.00\n",
      "Step: 35000\t# Episodes: 2\tAvg ep reward: 3050.00\n",
      "Step: 40000\t# Episodes: 2\tAvg ep reward: 3500.00\n",
      "Step: 45000\t# Episodes: 1\tAvg ep reward: 7000.00\n",
      "Step: 50000\t# Episodes: 2\tAvg ep reward: 8000.00\n",
      "Step: 55000\t# Episodes: 1\tAvg ep reward: 6400.00\n",
      "Step: 60000\t# Episodes: 2\tAvg ep reward: 9300.00\n",
      "Step: 65000\t# Episodes: 2\tAvg ep reward: 5650.00\n",
      "Step: 70000\t# Episodes: 1\tAvg ep reward: 6100.00\n",
      "Step: 75000\t# Episodes: 1\tAvg ep reward: 4000.00\n",
      "Step: 80000\t# Episodes: 2\tAvg ep reward: 7450.00\n",
      "Step: 85000\t# Episodes: 1\tAvg ep reward: 3600.00\n",
      "Step: 90000\t# Episodes: 2\tAvg ep reward: 8500.00\n",
      "Step: 95000\t# Episodes: 2\tAvg ep reward: 5800.00\n",
      "Step: 100000\t# Episodes: 1\tAvg ep reward: 10200.00\n",
      "Starting new training loop\n",
      "Step: 5000\t# Episodes: 1\tAvg ep reward: 2700.00\n",
      "Step: 10000\t# Episodes: 1\tAvg ep reward: 5800.00\n",
      "Step: 15000\t# Episodes: 2\tAvg ep reward: 2300.00\n",
      "Step: 20000\t# Episodes: 1\tAvg ep reward: 3400.00\n",
      "Step: 25000\t# Episodes: 2\tAvg ep reward: 3950.00\n",
      "Step: 30000\t# Episodes: 2\tAvg ep reward: 3600.00\n",
      "Step: 35000\t# Episodes: 2\tAvg ep reward: 3700.00\n",
      "Step: 40000\t# Episodes: 1\tAvg ep reward: 2000.00\n",
      "Step: 45000\t# Episodes: 1\tAvg ep reward: 3600.00\n",
      "Step: 50000\t# Episodes: 1\tAvg ep reward: 3400.00\n",
      "Step: 55000\t# Episodes: 2\tAvg ep reward: 5200.00\n",
      "Step: 60000\t# Episodes: 1\tAvg ep reward: 4000.00\n",
      "Step: 65000\t# Episodes: 2\tAvg ep reward: 6400.00\n",
      "Step: 70000\t# Episodes: 2\tAvg ep reward: 4000.00\n",
      "Step: 75000\t# Episodes: 2\tAvg ep reward: 4100.00\n",
      "Step: 80000\t# Episodes: 1\tAvg ep reward: 6800.00\n",
      "Step: 85000\t# Episodes: 1\tAvg ep reward: 6700.00\n",
      "Step: 90000\t# Episodes: 2\tAvg ep reward: 3400.00\n",
      "Step: 95000\t# Episodes: 1\tAvg ep reward: 6700.00\n",
      "Step: 100000\t# Episodes: 1\tAvg ep reward: 10100.00\n",
      "----------------\n",
      "Without representation learning\n",
      "----------------\n",
      "Starting new training loop\n",
      "Step: 5000\t# Episodes: 1\tAvg ep reward: 1200.00\n",
      "Step: 10000\t# Episodes: 2\tAvg ep reward: 3800.00\n",
      "Step: 15000\t# Episodes: 2\tAvg ep reward: 4950.00\n",
      "Step: 20000\t# Episodes: 1\tAvg ep reward: 4300.00\n",
      "Step: 25000\t# Episodes: 2\tAvg ep reward: 2900.00\n",
      "Step: 30000\t# Episodes: 1\tAvg ep reward: 2600.00\n",
      "Step: 35000\t# Episodes: 1\tAvg ep reward: 5900.00\n",
      "Step: 40000\t# Episodes: 2\tAvg ep reward: 6000.00\n",
      "Step: 45000\t# Episodes: 1\tAvg ep reward: 6300.00\n",
      "Step: 50000\t# Episodes: 1\tAvg ep reward: 8800.00\n",
      "Step: 55000\t# Episodes: 1\tAvg ep reward: 7600.00\n",
      "Step: 60000\t# Episodes: 1\tAvg ep reward: 10800.00\n",
      "Step: 65000\t# Episodes: 2\tAvg ep reward: 9700.00\n",
      "Step: 70000\t# Episodes: 1\tAvg ep reward: 7700.00\n",
      "Step: 75000\t# Episodes: 1\tAvg ep reward: 11500.00\n",
      "Step: 80000\t# Episodes: 2\tAvg ep reward: 11850.00\n",
      "Step: 85000\t# Episodes: 2\tAvg ep reward: 11750.00\n",
      "Step: 90000\t# Episodes: 2\tAvg ep reward: 11650.00\n",
      "Step: 95000\t# Episodes: 1\tAvg ep reward: 18300.00\n",
      "Step: 100000\t# Episodes: 1\tAvg ep reward: 23700.00\n",
      "Starting new training loop\n",
      "Step: 5000\t# Episodes: 1\tAvg ep reward: 3900.00\n",
      "Step: 10000\t# Episodes: 2\tAvg ep reward: 5500.00\n",
      "Step: 15000\t# Episodes: 1\tAvg ep reward: 4400.00\n",
      "Step: 20000\t# Episodes: 1\tAvg ep reward: 3800.00\n",
      "Step: 25000\t# Episodes: 2\tAvg ep reward: 3100.00\n",
      "Step: 30000\t# Episodes: 1\tAvg ep reward: 5300.00\n",
      "Step: 35000\t# Episodes: 3\tAvg ep reward: 3466.67\n",
      "Step: 40000\t# Episodes: 2\tAvg ep reward: 4250.00\n",
      "Step: 45000\t# Episodes: 1\tAvg ep reward: 5900.00\n",
      "Step: 50000\t# Episodes: 1\tAvg ep reward: 6100.00\n",
      "Step: 55000\t# Episodes: 2\tAvg ep reward: 7200.00\n",
      "Step: 60000\t# Episodes: 2\tAvg ep reward: 7350.00\n",
      "Step: 65000\t# Episodes: 2\tAvg ep reward: 11200.00\n",
      "Step: 70000\t# Episodes: 1\tAvg ep reward: 11700.00\n",
      "Step: 75000\t# Episodes: 0\tAvg ep reward: nan\n",
      "Step: 80000\t# Episodes: 0\tAvg ep reward: nan\n",
      "Step: 85000\t# Episodes: 0\tAvg ep reward: nan\n",
      "Step: 90000\t# Episodes: 0\tAvg ep reward: nan\n",
      "Step: 95000\t# Episodes: 0\tAvg ep reward: nan\n",
      "Step: 100000\t# Episodes: 0\tAvg ep reward: nan\n",
      "Starting new training loop\n",
      "Step: 5000\t# Episodes: 1\tAvg ep reward: 2000.00\n",
      "Step: 10000\t# Episodes: 1\tAvg ep reward: 3400.00\n",
      "Step: 15000\t# Episodes: 2\tAvg ep reward: 6200.00\n",
      "Step: 20000\t# Episodes: 1\tAvg ep reward: 3600.00\n",
      "Step: 25000\t# Episodes: 1\tAvg ep reward: 3700.00\n",
      "Step: 30000\t# Episodes: 1\tAvg ep reward: 4000.00\n",
      "Step: 35000\t# Episodes: 1\tAvg ep reward: 3800.00\n",
      "Step: 40000\t# Episodes: 1\tAvg ep reward: 3400.00\n",
      "Step: 45000\t# Episodes: 1\tAvg ep reward: 3400.00\n",
      "Step: 50000\t# Episodes: 2\tAvg ep reward: 2950.00\n",
      "Step: 55000\t# Episodes: 1\tAvg ep reward: 3400.00\n",
      "Step: 60000\t# Episodes: 1\tAvg ep reward: 3400.00\n",
      "Step: 65000\t# Episodes: 1\tAvg ep reward: 3500.00\n",
      "Step: 70000\t# Episodes: 2\tAvg ep reward: 3900.00\n",
      "Step: 75000\t# Episodes: 1\tAvg ep reward: 5400.00\n",
      "Step: 80000\t# Episodes: 2\tAvg ep reward: 6200.00\n",
      "Step: 85000\t# Episodes: 2\tAvg ep reward: 11050.00\n",
      "Step: 90000\t# Episodes: 1\tAvg ep reward: 11000.00\n",
      "Step: 95000\t# Episodes: 1\tAvg ep reward: 11000.00\n",
      "Step: 100000\t# Episodes: 0\tAvg ep reward: nan\n",
      "Starting new training loop\n",
      "Step: 5000\t# Episodes: 1\tAvg ep reward: 1900.00\n",
      "Step: 10000\t# Episodes: 1\tAvg ep reward: 3400.00\n",
      "Step: 15000\t# Episodes: 1\tAvg ep reward: 3400.00\n",
      "Step: 20000\t# Episodes: 1\tAvg ep reward: 3400.00\n",
      "Step: 25000\t# Episodes: 2\tAvg ep reward: 3400.00\n",
      "Step: 30000\t# Episodes: 1\tAvg ep reward: 3400.00\n",
      "Step: 35000\t# Episodes: 1\tAvg ep reward: 2700.00\n",
      "Step: 40000\t# Episodes: 1\tAvg ep reward: 4600.00\n",
      "Step: 45000\t# Episodes: 2\tAvg ep reward: 4100.00\n",
      "Step: 50000\t# Episodes: 1\tAvg ep reward: 5900.00\n",
      "Step: 55000\t# Episodes: 1\tAvg ep reward: 3700.00\n",
      "Step: 60000\t# Episodes: 2\tAvg ep reward: 3950.00\n",
      "Step: 65000\t# Episodes: 0\tAvg ep reward: nan\n",
      "Step: 70000\t# Episodes: 2\tAvg ep reward: 3600.00\n",
      "Step: 75000\t# Episodes: 2\tAvg ep reward: 4750.00\n",
      "Step: 80000\t# Episodes: 0\tAvg ep reward: nan\n",
      "Step: 85000\t# Episodes: 1\tAvg ep reward: 11000.00\n",
      "Step: 90000\t# Episodes: 3\tAvg ep reward: 7333.33\n",
      "Step: 95000\t# Episodes: 1\tAvg ep reward: 11300.00\n",
      "Step: 100000\t# Episodes: 1\tAvg ep reward: 13000.00\n",
      "Starting new training loop\n",
      "Step: 5000\t# Episodes: 1\tAvg ep reward: 3900.00\n",
      "Step: 10000\t# Episodes: 1\tAvg ep reward: 6400.00\n",
      "Step: 15000\t# Episodes: 2\tAvg ep reward: 3400.00\n",
      "Step: 20000\t# Episodes: 1\tAvg ep reward: 2900.00\n",
      "Step: 25000\t# Episodes: 1\tAvg ep reward: 3400.00\n",
      "Step: 30000\t# Episodes: 1\tAvg ep reward: 3400.00\n",
      "Step: 35000\t# Episodes: 1\tAvg ep reward: 3400.00\n",
      "Step: 40000\t# Episodes: 1\tAvg ep reward: 3500.00\n",
      "Step: 45000\t# Episodes: 1\tAvg ep reward: 3300.00\n",
      "Step: 50000\t# Episodes: 1\tAvg ep reward: 5800.00\n",
      "Step: 55000\t# Episodes: 1\tAvg ep reward: 4300.00\n",
      "Step: 60000\t# Episodes: 2\tAvg ep reward: 4050.00\n",
      "Step: 65000\t# Episodes: 1\tAvg ep reward: 3500.00\n",
      "Step: 70000\t# Episodes: 1\tAvg ep reward: 4700.00\n",
      "Step: 75000\t# Episodes: 0\tAvg ep reward: nan\n",
      "Step: 80000\t# Episodes: 1\tAvg ep reward: 7900.00\n",
      "Step: 85000\t# Episodes: 1\tAvg ep reward: 9200.00\n",
      "Step: 90000\t# Episodes: 2\tAvg ep reward: 9150.00\n",
      "Step: 95000\t# Episodes: 1\tAvg ep reward: 8100.00\n",
      "Step: 100000\t# Episodes: 1\tAvg ep reward: 11000.00\n"
     ]
    }
   ],
   "source": [
    "print('-'*16 + '\\nWith representation learning\\n' + '-'*16)\n",
    "for _ in range(5):\n",
    "    print('Starting new training loop')\n",
    "\n",
    "    embed_dim = 64\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    rainbow_args = copy.copy(DEFAULT_RAINBOW_ARGS)\n",
    "    rainbow_args.device = device\n",
    "    # rainbow_args.replay_frequency = 8\n",
    "\n",
    "    sf_model = SFNetwork(list(env.observation_space.shape), embed_dim)\n",
    "    repr_learner = SFPredictor(sf_model, lr=1e-4)\n",
    "\n",
    "    agent = RainbowAgent(rainbow_args, env, sf_model.encoder, repr_learner)\n",
    "    sf_model = sf_model.to(device)\n",
    "    repr_learner._update_target_model()\n",
    "\n",
    "    train_task_model(agent, env, int(1e5))\n",
    "\n",
    "print('-'*16 + '\\nWithout representation learning\\n' + '-'*16)\n",
    "for _ in range(5):\n",
    "    print('Starting new training loop')\n",
    "\n",
    "    embed_dim = 64\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    rainbow_args = copy.copy(DEFAULT_RAINBOW_ARGS)\n",
    "    rainbow_args.device = device\n",
    "    # rainbow_args.replay_frequency = 8\n",
    "\n",
    "    sf_model = SFNetwork(list(env.observation_space.shape), embed_dim)\n",
    "    repr_learner = SFPredictor(sf_model, lr=1e-4)\n",
    "\n",
    "    agent = RainbowAgent(rainbow_args, env, sf_model.encoder, None)\n",
    "    sf_model = sf_model.to(device)\n",
    "    repr_learner._update_target_model()\n",
    "\n",
    "    train_task_model(agent, env, int(1e5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent = TestRL(agent)\n",
    "# agent.start_task(1000)\n",
    "# obs = env.reset()\n",
    "# act = agent.sample_act(obs)\n",
    "# print('Act:', act)\n",
    "# obs, reward, done, _ = env.step(act)\n",
    "# agent.process_step_data((obs, act, reward, obs, done))\n",
    "# agent.end_step()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6717cf457fe527f2ad07ab71b4770f157b357bf37d07e7427487ba89b10c0212"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('ml2': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
